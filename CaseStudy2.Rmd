---
title: "Case Study 2 - Identify key attrition factors"
author: "Jonathan Flores, Melissa Luzardo, and Randy Park"
date: "April 16, 2018"
output: 
  md_document:
    variant: markdown_github
---
# Exploratory Data Analysis
The Exploratory Data Analysis (EDA) and extensive analysis have been employeed throughout this analysis and mainly consist of 3 main steps as below:

1. **Exploratory Data Analysis** 
   *  Load Attrition data
   *  Quick identification of all variables and visible trends
   *  Clean and transform the features to a numeric dataset
2. **Model Creation**
   *  Evaluate Prediciton Models against the requirements
   *  Split dataset into a training dataset and a test dataset
   *  Create a model and a reduced model
3. **Cross Validation**
   *  Create predictions using a test dataset
   *  Cross-validate with a test dataset
   *  Study Results

In addtion, this report should include an analyis of **job role specific trends** and recommended actions to attain talents.

``` {r echo=TRUE, comment=NA, results='asis'}
# Load requeried library
suppressWarnings(suppressMessages(library("readxl")))
suppressWarnings(suppressMessages(library(RCurl)))

# After unziping the provided datafile, the .xlsl document was loaded on GitHub, the code below process information from GitHub instead of our local computers.
download.file("https://raw.githubusercontent.com/cyberkoolman/msds.6306.case.study.2/master/Data/CaseStudy2-data.xlsx", "Data/data2.xlsx", mode="wb")

# Assign file to object 
case_data <- data.frame(read_excel("Data/data2.xlsx"))

```


## Clean data

First, Remove non-value added variables from the dataset

* **EmployeeCount**: Always 1, since the data set is by employee. 
* **Over18**: All employees are "Y". Age is a more meaningful and relevant variable. 
* **StandardHours**: All are "80".

```{r echo=TRUE, results='asis'}

# Remove redundandt info: EmployeeCount, Over18, StandardHours
df <- case_data[,-c(9,22,27)]

```

Second, Covert character variables to factors and factors to numeric variables as need

* To evaluate correlations we need to use numeric variables, and evaluate whether the correlations were positive or negative for character variables. Later we will use the factor variables for further analysis. Here we create two data frames, one with level plus numeric variables and the other with only numeric variables.


```{r echo=TRUE, results='asis'}
# Load required libraries.
suppressWarnings(suppressMessages(library(purrr)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(pander)))

# Convert characters to factors.
df %>% map_if(is.character, as.factor) %>% as_data_frame -> df

#Adjust factor levels as needed
levels(df$BusinessTravel)<-c("Non-Travel","Travel_Rarely","Travel_Frequently")

#Make all variable numeric
numdf<-data.frame(sapply(df,as.numeric))

```

## Simple Analysis of Attrition plus result of Univaratite Analysis
Quick glance of the attrition count and percentage
``` {r echo=TRUE,  results='asis'}

#Calculate attrition in data set
AT <- table(df$Attrition)
pander(AT)

#And percentages
at<-data.frame(table(df$Attrition))
pander(prop.table(at[,2]))
```

Attrition (%) | Retaining (%)
------------- | -------------
16.1 2%       | 83.88 %

**Outcome of Univariate Analysis**

The team has attempted on **various univariate analysis** to find the key attrition factors.  After the analysis, **it was evident to explore using different methodologies**.  Since the analysis were quite long and comprehensive in its own merit, the team has separated its analysis in the separate R markdown file. [Please find the univarate findings in this markdown draft for detailed analysis](https://github.com/cyberkoolman/msds.6306.case.study.2/blob/master/Univariate/univariate_draft.md).


## Correlation coeffientes between parameters and attrition
Calculate correlation coefficients to locate highest correating coeffients with Attrition

``` {r echo=TRUE,  results='asis'}

#Correlate variables
Attcor<-data.frame(cor(numdf))

#Create Attrition object for Attrition correlation coefficients 
Attrition<- data.frame(Attcor$Attrition)

#Name attrition rows
Attrition$Parameter<-row.names(Attcor)

#Rename titles Attrition
names(Attrition)<-c("Correlation", "Parameter")

#Sort positive Attrition
SortAtt <- Attrition[order(-Attrition$Correlation),]

#Display top 10 Positively Correlated Parameters
row.names(SortAtt)<-NULL
knitr::kable(head(SortAtt,10))

```

## Visualize coeffiecients and correlations
```{r echo=TRUE,  results='asis'}

# Load library to visualize correlations
suppressWarnings(suppressMessages(library(corrplot)))

#Display graphic with all correlations
par(cex=.5)
corrplot(as.matrix(Attcor), method="color", 
         type="upper", 
         addCoef.col = "black",
         tl.col="black", tl.srt=45,
         sig.level = 0.05, insig = "blank",
         title="Figure 1.1 Complete Coefficients and Correlations", 
         diag=FALSE)

#Display graphic with the top 10 positively correlated parameter
par(cex=.8)
SA10<-c(head(SortAtt$Parameter,10))
corrplot(as.matrix(Attcor[SA10,SA10]), method="pie", 
         type="upper", 
         addCoef.col = "black",
         tl.col="black", tl.srt=45,
         sig.level = 0.05, insig = "blank", 
         diag=FALSE)

# Further look at top 10 positively correlated parameters with Attrition
par(mfrow=c(3,3), las=2)
plot(Attrition~OverTime, data=df, main="Attrition vs Overtime")
plot(Attrition~MaritalStatus, data=df, main="Attrition vs Marital Status")
plot(Attrition~DistanceFromHome, data=df, main="Attrition vs Distance From Home")
plot(Attrition~JobRole, data=df, main="Attrition vs Job Role")
plot(Attrition~Department, data=df, main="Attrition vs Department")
plot(Attrition~NumCompaniesWorked, data=df, main= "Attrition vs Number of Companies Worked")
plot(Attrition~Gender, data=df, main= "Attrition vs Gender" )
plot(Attrition~EducationField, data=df, main="Attrition vs Eduaction Field")
plot(Attrition~MonthlyRate, data=df, main="Attrition vs Monlthly Rate")

```

## Absolute values of the highly correlated parameters with attrition
Because the level variables were converted to numeric variables, it is possible that strong correlations are reported as negative correlations. To account for this fact, we look at the correlation absolute values as well.

```{r echo=TRUE, comment=NA, results='asis'}

##Calculate absolute value coefficients
Attrition$AbsAtt <- (Attrition$Correlation^2)^(1/2)
SortAbstAtt<- Attrition[order(-Attrition$AbsAtt),]

#Display top 10 Absolute Correlated Parameters
row.names(SortAbstAtt)<-NULL
knitr::kable(head(SortAbstAtt,10))

#Display graphic with top 10 Absolute Correlated Parameters
par(cex=.8)
SAA10<-c(head(SortAbstAtt$Parameter,10))
corrplot(as.matrix(Attcor[SAA10,SAA10]), method="pie", 
         type="upper", 
         addCoef.col = "black",
         tl.col="black", tl.srt=45,
         sig.level = 0.05, insig = "blank", 
         diag=FALSE)

#Further look at top 10 absolute correlated parameters with Attrition
par(mfrow=c(3,3), las=2)
plot(Attrition~OverTime, data=df, main="Attrition vs Overtime")
plot(Attrition~TotalWorkingYears, data=df, main="Attrition vs Total Working Years")
plot(Attrition~JobLevel, data=df, main="Attrition vs Job Level")
plot(Attrition~ MaritalStatus, data=df, main="Attrition vs Marital Status")
plot(Attrition~YearsInCurrentRole, data=df, main="Attrition vs Years in Current Role")
plot(Attrition~MonthlyIncome, data=df, main="Attrition vs Monthly Income")
plot(Attrition~Age, data=df, main="Attrition vs Age")
plot(Attrition~YearsWithCurrManager, data=df, main="Attrition vs Years with Current Manager")
plot(Attrition~StockOptionLevel, data=df, main="Attrition vs Stock Option Level")
```

# Job Role Specific Trends
First, we look at the Job Role and its relation with attrition, then look at correlations between the other parameters and the Job Role
```{r echo=TRUE, comment=NA, results='asis'}

# Display Job Role and Attrition correlation bar graph
par(mar=c(12, 5, 5, 2.1),mgp=c(10, 1, 0),las=2)
plot(Attrition~JobRole, data=df, main="Attrition vs. Job Role")

#Generate object for Job Role correlation coefficients 
JobRole<- data.frame(Attcor$JobRole)

#Name JobRole rows
JobRole$Parameter<-row.names(Attcor)

#Rename titles Attrition
names(JobRole)<-c("Correlation", "Parameter")

##Calculate absolute value coefficients
JobRole$Abs <- (JobRole$Correlation^2)^(1/2)
SortJobRole<- JobRole[order(-JobRole$Abs),]

#Display top 5 Absolute Correlated Parameters
row.names(SortJobRole)<-NULL
knitr::kable(head(SortJobRole))

#Display graphic with top 4 Absolute Correlated Parameters
par(cex=.8)
SJR5<-c(head(SortJobRole$Parameter,5))
corrplot(as.matrix(Attcor[SJR5,SJR5]), method="pie", 
         type="upper", 
         addCoef.col = "black",
         tl.col="black", tl.srt=45,
         sig.level = 0.05, insig = "blank", 
         diag=FALSE)

#Further look at top 4 absolute correlated parameters with Job ROle
par(las=2)
par(mar=c(12, 12, 5, 2.1),mgp=c(10, 1, 0),las=2)
plot(TotalWorkingYears~JobRole, data=df, main="Total Working Years vs Job Role")
plot(Age~JobRole, data=df, main="Age vs Job Role")
plot(MonthlyIncome~JobRole, data=df, main= "Montly Income vs Job Role")

```

# Logistic Regression Model

## Creating Sampling Set

Generating traing and testing data as sampling set

```{r echo=TRUE, comment=NA, results='asis'}

# Converting numeric variables (of level parametets) to factors as needed
LM <- df
LM$Education <- as.factor(LM$Education)
LM$EnvironmentSatisfaction <- as.factor(LM$EnvironmentSatisfaction)
LM$JobInvolvement <- as.factor(LM$JobInvolvement)
LM$JobSatisfaction <- as.factor(LM$JobSatisfaction)
LM$PerformanceRating <- as.factor(LM$PerformanceRating)
LM$RelationshipSatisfaction <- as.factor(LM$RelationshipSatisfaction)
LM$WorkLifeBalance <- as.factor(LM$WorkLifeBalance)

## Creating a Training and Testing data set from sampling as a random 80% of the data set provided
smp<-floor(0.9*nrow(LM))
set.seed(123)
ind <- sample(seq_len(nrow(LM)),size=smp)
train <- LM[ind,]
test <- LM[-ind,]
```

## Start with a preliminary model
Start with a preliminary model with all the variables

```{r echo=TRUE, comment=NA, results='asis'}

## Load requiered libraries

library(psych)
library(pander)

#Logistic Regression
model <- glm(Attrition ~ ., family = 'binomial', data = train)

pander(model)

cat(model$aic)

```

## Refine model - Reduced Model
Refine Logistic Regression Model by removing not statistically significant variables (p-value >0.05) to reduce AIC value.

```{r echo=TRUE, comment=NA, results='asis'}

## Model 2 

model2 <- glm(Attrition ~ Age + BusinessTravel + DistanceFromHome + EnvironmentSatisfaction +   JobInvolvement + JobRole + JobSatisfaction + MaritalStatus + NumCompaniesWorked + OverTime +   RelationshipSatisfaction + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager, family = 'binomial', data = train)

pander(model2)

#run anova on model2

pander(anova(model2, test = 'Chisq'))

cat(model2$aic)

```

## Evaluate top 3 parameters
Evaluate Logistic Regression Model with top 3 statisticaly significant variables (p-value <0.05) 

```{r echo=TRUE, comment=NA, results='asis'}

# Model 3 - Logistic Regression Model with the top 3 statistially significant parameters 

model3 <- glm(Attrition ~ JobSatisfaction + MaritalStatus + OverTime , family = 'binomial', data = train)

cat(model3$aic)

```

## Reduced Model Evaluation
```{r echo=TRUE, comment=NA, results='asis'}

#Prediction for the test data

predmodel <- round(predict(model2,test,type='response'),digits=0)
comparison <- data.frame(predmodel,test$Attrition,test$EmployeeNumber)
names(comparison)<- c("Predicted","Actual","EmployeeNumber")
pred <- ifelse(predmodel>0.5,2,1)

#Loading required libraries
library(ROCR)
library(Metrics)
library(pROC)

pr <- prediction(pred,test$Attrition)
perf <- performance(pr,measure = "tpr",x.measure = "fpr")
plot(perf)
auc(test$Attrition,pred)

```

## Prediction probabilities and results in comparison with actuals

```{r echo=TRUE, comment=NA, results='asis'}
#Prediction probabilities and results in comparison with actuals

comparison2 <- cbind(predmodel,pred,test$Attrition,test$EmployeeNumber)
colnames(comparison2) <- c("Probability Greater Than .5 is Yes and Lower than .5 is No ", "Prediction - 2 Yes and 1 is No","Actual - 2 Yes and 1 is No","Employee Number")

#Missclassification error 
tabb <- table(Predicted = pred, Actual = test$Attrition)
row.names(tabb) <- c("total number","total missclassifications")
knitr::kable(head(tabb))

#Missclassification percentage
cat(1-sum(diag(tabb))/sum(tabb))

#Goodness of fit test

cat(with(model2,pchisq(null.deviance-deviance,df.null-df.residual,lower.tail = F)))

```

# Executive Summary
After cleaning up the file, the variables were characterized as ether numbers or parameters with levels. The levels were then transformed to numbers to find correlations with the Attrition. Once those correlations were calculated, the correlation coefficients were sorted from highest to lowest. The top 3 parameters with the highest positive correlation coefficients were:

1.	Overtime  
2.	Marital Status  
3.	Distance from Home  

Since two of these variables were factors converted to numbers a visual evaluation was recommended. Also, it was important to consider the absolute values of the correlations. Since several variables were factors. When studying the correlation bar graphs, there is a clear correlation between the first two parameters, over time and marital status with attrition, whereas the distance from home is not as evident. Regarding the usefulness of this information, the company can probably address overtime. Marital status and distance from home are a little more difficult. 
When evaluating the absolute value of the correlations, the top three parameters are:

1.	Overtime  
2.	Total working years 
3.	Job Level 

Here it is clear that the negative correlation provides useful information because identifies a strong correlation between the few years" total working years", "job level" and "years with current manager" and attrition. Another interesting trend can be observed in the "years in current role". Where the trend shows a trend, looking like there is probably an expectation of promotion or salary increase. The data obtain here is data from variables that can be address from an organizational perspective. Having few total working years and a low job level might be related but required further analysis. Employees with few total working years and a low job level might be an easily trained or target group for retention. Other variables showed similar trends and similar behavior, years in current role, years with current manager, and monthly income.   
Another question looking to answer were trends related to job specific roles. When looking at the job roles. We get that Role with the highest attrition is Sales Representative and the one with the least is the Research Director. And in agreement with the previously discuss observations the Sales Representatives are, on average, the youngest with the less total working years and the lower monthly income. 
As data scientist the team provided a logistic regression model to help predict the probability of an employee to leave the company. The model requires refining with a data learning algorithm but at the moment presets an approximate accuracy of 70% AUC-ROC with a miscalculation of about 14% of the data provided.   The model was developed with a train test ratio of 90% of the data set. 

From the logistic regression model, the parameters with the highest logistic regression coefficients were 

1.	Overtime  
2.	Job Satisfaction  
3.	Marital Status

The first parameter (overtime) presents itself as an overwhelming factor as for the second and third the models disagree. Job satisfaction is definitively an important parameter and if an employee satisfaction level is low the employee is more likely to leave than otherwise. However, the number of employees that leave and report low job satisfaction is significantly lower than the single employees. That said, the company can address employee satisfaction but can do little about the employee marital status. The regression model is intended to help identify areas of improvement and way to lower attrition rates.

# Conclusion

The data science team identified 

1.	Overtime  
2.	Total working years 
3.	Job Level 

As the top three factor that lead to turnover.And observed that the Sales Representative were the most sensitive Job Role to attrition affected with factors such as total working years, job level and age.

Ref.:
https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/